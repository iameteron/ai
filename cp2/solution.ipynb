{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cd71a6",
   "metadata": {},
   "source": [
    "## Курсовая работа по курсу ФКИИ\n",
    "\n",
    "Выполнил cтудент группы М8О-109СВ-24, Сорокин Никита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from captum.attr import InputXGradient\n",
    "from lime import lime_image\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.transform import resize\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # Панацея от отвала ядра "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe76242",
   "metadata": {},
   "source": [
    "## Скачивание небольшого датасета с картинками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "!curl -L -o cats-dogs-datset.zip https://www.kaggle.com/api/v1/datasets/download/samuelcortinhas/cats-and-dogs-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"cats-dogs-datset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"dataset\")\n",
    "print(\"Распаковка завершена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6f8a4",
   "metadata": {},
   "source": [
    "### Деление датасета на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5044200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Исходная папка с подкатегориями\n",
    "source_dir = 'dataset/dataset'\n",
    "target_root = 'dataset_split'  # Новая папка для train/val/test\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "split_ratios = {'train': 0.8, 'val': 0.1, 'test': 0.1}\n",
    "\n",
    "classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "\n",
    "for cls in classes:\n",
    "    cls_dir = os.path.join(source_dir, cls)\n",
    "    images = os.listdir(cls_dir)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    n_total = len(images)\n",
    "    n_train = int(n_total * split_ratios['train'])\n",
    "    n_val = int(n_total * split_ratios['val'])\n",
    "    \n",
    "    split_data = {\n",
    "        'train': images[:n_train],\n",
    "        'val': images[n_train:n_train+n_val],\n",
    "        'test': images[n_train+n_val:]\n",
    "    }\n",
    "\n",
    "    for split in splits:\n",
    "        split_dir = os.path.join(target_root, split, cls)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        for img_name in split_data[split]:\n",
    "            src = os.path.join(cls_dir, img_name)\n",
    "            dst = os.path.join(split_dir, img_name)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(\"Разделение завершено: train / val / test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0525774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "plt.ion()\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = './dataset_split'\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                   shuffle=True if x != 'test' else False,\n",
    "                                   num_workers=4)\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b233e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec087bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './dataset_split'\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "def imshow(inp_tensor, title=None):\n",
    "    try:\n",
    "        inp = inp_tensor.detach().cpu().numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.imshow(inp)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.pause(0.001)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Ошибка в imshow:\", e)\n",
    "\n",
    "try:\n",
    "    inputs, class_ids = next(iter(dataloaders['train']))\n",
    "    inputs = inputs[:2]\n",
    "    class_ids = class_ids[:2]\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "    title = ' | '.join([class_names[x] for x in class_ids])\n",
    "    imshow(out, title=title)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"Dataloader пуст — проверь содержимое dataset_split/train/\")\n",
    "except Exception as e:\n",
    "    print(\"Ошибка при визуализации батча:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c6b0c",
   "metadata": {},
   "source": [
    "# Натренируем модельку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "            \n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp_tensor, title=None):\n",
    "    \"\"\"Отображение тензора изображения с денормализацией.\"\"\"\n",
    "    inp = inp_tensor.detach().cpu().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cdfbf1",
   "metadata": {},
   "source": [
    "## Проверки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5963316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(model,img_path):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    img = data_transforms['val'](img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        ax = plt.subplot(2,2,1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
    "        imshow(img.cpu().data[0])\n",
    "\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "visualize_model_predictions(\n",
    "    model_conv,\n",
    "    img_path='./img/first picture.jpg'\n",
    ")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d1330",
   "metadata": {},
   "source": [
    "# LIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeebf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image_for_model(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = data_transforms['val'](img)\n",
    "    return img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    return tensor.cpu() * std + mean\n",
    "\n",
    "def batch_predict(images):\n",
    "    model_conv.eval()\n",
    "    batch = torch.stack([data_transforms['val'](Image.fromarray(img)).to(device) for img in images], dim=0)\n",
    "    with torch.no_grad():\n",
    "        logits = model_conv(batch)\n",
    "    return torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "def explain_prediction_with_lime(img_path):\n",
    "    img = Image.open(img_path).convert('RGB').resize((256, 256))\n",
    "    np_img = np.array(img)\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    explanation = explainer.explain_instance(\n",
    "        np_img,\n",
    "        batch_predict,\n",
    "        top_labels=1,\n",
    "        hide_color=0,\n",
    "        num_samples=1000\n",
    "    )\n",
    "\n",
    "    temp, mask = explanation.get_image_and_mask(\n",
    "        label=explanation.top_labels[0],\n",
    "        positive_only=True,\n",
    "        hide_rest=False,\n",
    "        num_features=10,\n",
    "        min_weight=0.0\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(f\"Predicted: {class_names[explanation.top_labels[0]]}\")\n",
    "    plt.imshow(mark_boundaries(temp / 255.0, mask))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction_with_lime(\"./img/first picture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction_with_lime(\"./img/second picture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction_with_lime(\"./img/third picture.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5804fb0",
   "metadata": {},
   "source": [
    "# SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fd3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_with_shap(model, img_path, device, class_names):\n",
    "    def patched_forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "    BasicBlock.forward = patched_forward\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    class_id = output.argmax(dim=1).item()\n",
    "    print(f\"Предсказанный класс: {class_names[class_id]}\")\n",
    "\n",
    "    model_cpu = model.cpu()\n",
    "    for m in model_cpu.modules():\n",
    "        if isinstance(m, torch.nn.ReLU):\n",
    "            m.inplace = False\n",
    "\n",
    "    img_tensor = img_tensor.cpu().detach().requires_grad_()\n",
    "    from torchvision.transforms import GaussianBlur\n",
    "    blur = GaussianBlur(kernel_size=15)\n",
    "    background = blur(img_tensor)\n",
    "\n",
    "    explainer = shap.GradientExplainer(model_cpu, background)\n",
    "    shap_vals = explainer.shap_values(img_tensor)\n",
    "\n",
    "    shap_img = shap_vals[0][:, :, :, 0]  # (3, H, W)\n",
    "    if shap_img.ndim == 3:\n",
    "        shap_map = np.mean(shap_img, axis=0)\n",
    "    else:\n",
    "        shap_map = shap_img\n",
    "\n",
    "    def denorm(img):\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        return torch.clamp(img * std + mean, 0, 1).detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    img_vis = denorm(img_tensor[0])\n",
    "    norm = np.clip((shap_map / (np.percentile(np.abs(shap_map), 98) + 1e-6) + 1) / 2, 0, 1)\n",
    "    heatmap = plt.cm.seismic(norm)[..., :3]\n",
    "    if heatmap.shape[:2] != img_vis.shape[:2]:\n",
    "        heatmap = resize(heatmap, img_vis.shape[:2], preserve_range=True)\n",
    "    overlay = np.clip(0.6 * heatmap + 0.4 * img_vis, 0, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    ax[0].imshow(img_vis)\n",
    "    ax[0].set_title(\"Оригинал\")\n",
    "    ax[1].imshow(overlay)\n",
    "    ax[1].set_title(\"SHAP Overlay\")\n",
    "    vlim = np.percentile(np.abs(shap_map), 98)\n",
    "    ax[2].imshow(np.abs(shap_map), cmap='hot', vmin=0, vmax=vlim, aspect='auto')\n",
    "    ax[2].set_title(\"|SHAP|\")\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_shap(model_conv, \"./img/first picture.jpg\", device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_shap(model_conv, \"./img/second picture.jpg\", device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_shap(model_conv, \"./img/third picture.jpg\", device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def94c6",
   "metadata": {},
   "source": [
    "# Captum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(3, 1, 1)\n",
    "    return torch.clamp(tensor * std + mean, 0, 1)\n",
    "\n",
    "def create_heatmap_overlay(img: torch.Tensor, at_2d: np.ndarray, alpha: float = 0.6) -> np.ndarray:\n",
    "    img_np = denormalize_image(img.cpu().detach()).numpy().transpose(1, 2, 0)\n",
    "    h, w, _ = img_np.shape\n",
    "\n",
    "    at_2d = at_2d.astype(np.float32)\n",
    "    at_2d = np.nan_to_num(at_2d)\n",
    "    abs_max = np.percentile(np.abs(at_2d), 98)\n",
    "    if abs_max == 0:\n",
    "        abs_max = 1e-8\n",
    "    norm = np.clip(at_2d / abs_max, -1, 1)\n",
    "    norm = (norm + 1) / 2\n",
    "    heatmap = plt.cm.seismic(norm)[..., :3]\n",
    "\n",
    "    overlay = np.clip((1 - alpha) * img_np + alpha * heatmap, 0, 1)\n",
    "    return overlay\n",
    "\n",
    "def explain_with_captum(model, img_path, device, class_names):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    img_np = np.array(img.resize((224, 224))).astype(np.float32) / 255.0\n",
    "\n",
    "    model.eval()\n",
    "    img_tensor.requires_grad = True\n",
    "\n",
    "    output = model(img_tensor)\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "    print(f\"Предсказанный класс: {class_names[predicted_class]}\")\n",
    "\n",
    "    explainer = InputXGradient(model)\n",
    "    attributions = explainer.attribute(img_tensor, target=predicted_class)\n",
    "    attr_np = attributions.squeeze().detach().cpu().numpy()\n",
    "    at_gray = np.mean(attr_np, axis=0)\n",
    "\n",
    "    overlay = create_heatmap_overlay(img_tensor[0], at_gray, alpha=0.6)\n",
    "\n",
    "    vlim = np.percentile(np.abs(at_gray), 95)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title('Оригинал')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(overlay)\n",
    "    axes[1].set_title('Captum Heatmap')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(at_gray, cmap='seismic', vmin=-vlim, vmax=vlim)\n",
    "    axes[2].set_title('Raw Attribution')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    axes[3].imshow(np.abs(at_gray), cmap='hot')\n",
    "    axes[3].set_title('|Attribution|')\n",
    "    axes[3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a488823",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_captum(model_conv, \"./img/first picture.jpg\", device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_captum(model_conv, \"./img/second picture.jpg\", device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22376c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_captum(model_conv, \"./img/third picture.jpg\", device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a652eb5",
   "metadata": {},
   "source": [
    "# GradCam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_prediction_with_gradcam(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    input_tensor = data_transforms['val'](img).unsqueeze(0).to(device)\n",
    "    \n",
    "    img_np = np.array(img.resize((224, 224))).astype(np.float32) / 255.0\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model_conv.train()\n",
    "\n",
    "    target_layers = [model_conv.layer4[-1]]\n",
    "\n",
    "    cam = GradCAM(model=model_conv, target_layers=target_layers)\n",
    "\n",
    "    with torch.enable_grad(): \n",
    "        output = model_conv(input_tensor)\n",
    "        pred_class = output.argmax().item()\n",
    "\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class)])\n",
    "\n",
    "    cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cam_image)\n",
    "    plt.title(f\"Grad-CAM — класс: {class_names[pred_class]}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_on_random_val_images(n=10):\n",
    "    model_conv.train() \n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    indices = random.sample(range(len(image_datasets['val'])), n)\n",
    "\n",
    "    target_layers = [model_conv.layer4[-1]]\n",
    "    cam = GradCAM(model=model_conv, target_layers=target_layers)\n",
    "\n",
    "    for idx in indices:\n",
    "        img_tensor, label = image_datasets['val'][idx]\n",
    "        input_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        img_np = denormalize(img_tensor).numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            output = model_conv(input_tensor)\n",
    "            pred_class = output.argmax().item()\n",
    "\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class)])\n",
    "\n",
    "        cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(cam_image)\n",
    "        plt.title(f\"Predicted: {class_names[pred_class]}\\nTrue: {class_names[label]}\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fa1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_on_random_val_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction_with_gradcam(\"./img/first picture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction_with_gradcam(\"./img/second picture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f374087",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction_with_gradcam(\"./img/third picture.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
